{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HDyIP_dSifg4AS7E5J-sFNiud1Jep4AF",
      "authorship_tag": "ABX9TyOo9KfABlFYZOb2Y2mTAoi2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/The-DigitalAcademy/TrendHunt-Application/blob/Model/final_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7hTmercsFWAJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential  # Import the Sequential class\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define data directories\n",
        "train_data_dir = '/content/drive/MyDrive/my data/train'\n",
        "validation_data_dir = '/content/drive/MyDrive/my data/validation'\n",
        "test_data_dir = '/content/drive/MyDrive/my data/test'"
      ],
      "metadata": {
        "id": "GprPlAkuHwrz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data preprocessing parameters\n",
        "batch_size = 32\n",
        "input_shape = (150, 150)  # Adjust to your image size"
      ],
      "metadata": {
        "id": "2dfpdiBSILfH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation for training data (optional)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "L8NnpbQgIUjE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=input_shape,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'  # This will generate one-hot encoded labels\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z58rwDJ6IelR",
        "outputId": "d19abf47-e19f-4a0f-9aeb-bc2eee1e783f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 69 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=input_shape,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYUP-5y0IkqU",
        "outputId": "0f03011a-d552-4b89-f3c8-c8b6c425f980"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 70 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess test data (without data augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=input_shape,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b30cWZbpIvzo",
        "outputId": "15e1fe50-d2af-4036-d680-1a704d94ed3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 0 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the number of classes\n",
        "num_classes = len(train_generator.class_indices)\n",
        "num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNX5upqkJAA2",
        "outputId": "41df16fa-3347-46a0-fb2c-d9a592690d0d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model building"
      ],
      "metadata": {
        "id": "aABGzVScJcGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "Pq_wxwYUJelX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "UNUYOk4pJ7H4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIk_PsJFJ2KL",
        "outputId": "283c6efe-7984-40e5-d2b4-aa01ee34736c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 5s 4s/step - loss: 1.6918 - accuracy: 0.3514 - val_loss: 1.7375 - val_accuracy: 0.2500\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.6939 - accuracy: 0.2812 - val_loss: 1.7437 - val_accuracy: 0.2031\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 5s 4s/step - loss: 1.7181 - accuracy: 0.2432 - val_loss: 1.7174 - val_accuracy: 0.2188\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.6943 - accuracy: 0.2162 - val_loss: 1.7239 - val_accuracy: 0.2969\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 4s 3s/step - loss: 1.7153 - accuracy: 0.2031 - val_loss: 1.7158 - val_accuracy: 0.2656\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.7238 - accuracy: 0.2969 - val_loss: 1.6934 - val_accuracy: 0.2812\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 3s 3s/step - loss: 1.6826 - accuracy: 0.2432 - val_loss: 1.6904 - val_accuracy: 0.2344\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 5s 5s/step - loss: 1.7020 - accuracy: 0.1892 - val_loss: 1.6828 - val_accuracy: 0.2969\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.6529 - accuracy: 0.2656 - val_loss: 1.6958 - val_accuracy: 0.2656\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.6869 - accuracy: 0.1892 - val_loss: 1.7190 - val_accuracy: 0.2344\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 6s 3s/step - loss: 1.6476 - accuracy: 0.2969 - val_loss: 1.6895 - val_accuracy: 0.2656\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.8138 - accuracy: 0.2973 - val_loss: 1.7063 - val_accuracy: 0.2344\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 3s 3s/step - loss: 1.6614 - accuracy: 0.2432 - val_loss: 1.7235 - val_accuracy: 0.2031\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.6640 - accuracy: 0.2973 - val_loss: 1.7162 - val_accuracy: 0.2188\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 3s 2s/step - loss: 1.7040 - accuracy: 0.2162 - val_loss: 1.7366 - val_accuracy: 0.1875\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 3s 3s/step - loss: 1.7020 - accuracy: 0.1351 - val_loss: 1.7193 - val_accuracy: 0.2500\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 5s 2s/step - loss: 1.5645 - accuracy: 0.4054 - val_loss: 1.7701 - val_accuracy: 0.2969\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 4s 2s/step - loss: 1.6044 - accuracy: 0.3281 - val_loss: 1.7688 - val_accuracy: 0.2188\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 3s 1s/step - loss: 1.5878 - accuracy: 0.2432 - val_loss: 1.7456 - val_accuracy: 0.2656\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 6s 5s/step - loss: 1.6761 - accuracy: 0.2162 - val_loss: 1.7936 - val_accuracy: 0.2344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature vectors for all images in the dataset\n",
        "feature_vectors = model.predict(train_generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgvFqmjtNKiH",
        "outputId": "c195a7e5-d6e9-42e2-f0a8-f4d981cec994"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 221ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "def preprocess_query_image(image_path):\n",
        "    # Load the image\n",
        "    img = load_img(image_path, target_size=(150, 150))\n",
        "    # Convert the image to a NumPy array\n",
        "    img_array = img_to_array(img)\n",
        "    # Normalize the pixel values to the range [0, 1]\n",
        "    img_array = img_array / 255.0\n",
        "    return img_array\n"
      ],
      "metadata": {
        "id": "Lr1QUKNZNjkK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your query image\n",
        "query_image_path = '/content/drive/MyDrive/my data/test/fa064993-f459-457a-bdfb-69a2832e15da.jpg'\n",
        "\n",
        "# Example query image (you should preprocess it the same way as the training images)\n",
        "query_image = preprocess_query_image(query_image_path)\n",
        "\n",
        "# Extract feature vector for the query image\n",
        "query_features = model.predict(np.expand_dims(query_image, axis=0))  # Expand dimensions for a single image\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarities = cosine_similarity(query_features, feature_vectors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyzVpyvZOoxb",
        "outputId": "da4fa634-8b54-4c5c-89ca-f863c5419a93"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 97ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing"
      ],
      "metadata": {
        "id": "tSzYz40kPLwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sample images\n",
        "sample_image_paths = ['/content/drive/MyDrive/my data/test/fa13626d-572c-428f-ac1c-c40e1673195b.jpg', '/content/drive/MyDrive/my data/test/fa633eb5-0ca4-4e22-943e-1cdeaf9e60b5.jpg']\n",
        "\n",
        "# Preprocess sample images\n",
        "sample_images = [preprocess_query_image(image_path) for image_path in sample_image_paths]\n"
      ],
      "metadata": {
        "id": "ylWxsz_yPNXo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "sample_predictions = model.predict(np.array(sample_images))\n",
        "\n",
        "# Convert class probabilities to class labels (if needed)\n",
        "predicted_labels = np.argmax(sample_predictions, axis=1)\n",
        "predicted_labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG3u-7ZZPaQF",
        "outputId": "a0ee56f7-f362-45ca-9b70-d2ccecae2821"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 41ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class labels (replace with your own class labels)\n",
        "class_labels = ['Shirt', 'Dresses', 'Shoes','Skirts', 'T-shirt', 'Trouser']\n",
        "\n",
        "# Display predictions for each sample image\n",
        "for i in range(len(sample_image_paths)):\n",
        "    print(f\"Image: {sample_image_paths[i]}\")\n",
        "    print(f\"Predicted Class: {class_labels[predicted_labels[i]]}\")\n",
        "    print(f\"Class Probabilities: {sample_predictions[i]}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh_BaS3cPtnJ",
        "outputId": "7986825c-06bf-4fc6-a253-28b4c9753288"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: /content/drive/MyDrive/my data/test/fa13626d-572c-428f-ac1c-c40e1673195b.jpg\n",
            "Predicted Class: Skirts\n",
            "Class Probabilities: [0.12872005 0.05551829 0.17870551 0.30298096 0.08046499 0.2536102 ]\n",
            "\n",
            "Image: /content/drive/MyDrive/my data/test/fa633eb5-0ca4-4e22-943e-1cdeaf9e60b5.jpg\n",
            "Predicted Class: Shirt\n",
            "Class Probabilities: [0.36460352 0.2061712  0.04295815 0.05211071 0.2686395  0.06551698]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}